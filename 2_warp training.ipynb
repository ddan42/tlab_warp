{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9047267,"sourceType":"datasetVersion","datasetId":5454779},{"sourceId":189913772,"sourceType":"kernelVersion"},{"sourceId":191255049,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport copy\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n\nfrom datasets import Dataset, load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:04.553831Z","iopub.execute_input":"2024-08-05T11:06:04.554419Z","iopub.status.idle":"2024-08-05T11:06:07.659409Z","shell.execute_reply.started":"2024-08-05T11:06:04.554380Z","shell.execute_reply":"2024-08-05T11:06:07.658658Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:07.661080Z","iopub.execute_input":"2024-08-05T11:06:07.661518Z","iopub.status.idle":"2024-08-05T11:06:07.665768Z","shell.execute_reply.started":"2024-08-05T11:06:07.661493Z","shell.execute_reply":"2024-08-05T11:06:07.664823Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"config = {\n    'I_iterations': 2,\n    'M_models': 2,\n    'T_steps': 100, \n    'mu': 0.01,\n    'lmbd': 0.5,\n    'eta': 0.5,\n    '_beta': 1.0,\n    'batch_size': 64,\n    'train_first_n': 10,\n    'test_frist_n': 10,\n    'max_length': 50, # bert's max is 512, but it needs quite a lot of memory\n    'temperature': 0.9\n}\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:07.666980Z","iopub.execute_input":"2024-08-05T11:06:07.667350Z","iopub.status.idle":"2024-08-05T11:06:07.704362Z","shell.execute_reply.started":"2024-08-05T11:06:07.667305Z","shell.execute_reply":"2024-08-05T11:06:07.703587Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_parquet('/kaggle/input/imdb-csv/Train Data.parquet')\ntest_df = pd.read_parquet('/kaggle/input/imdb-csv/Test Data.parquet')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:13.992425Z","iopub.execute_input":"2024-08-05T11:06:13.993026Z","iopub.status.idle":"2024-08-05T11:06:14.238974Z","shell.execute_reply.started":"2024-08-05T11:06:13.992998Z","shell.execute_reply":"2024-08-05T11:06:14.238158Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# TODO CHECK train a model to generate positive reviews\ntrain_df = train_df[train_df['label'] == 1].reset_index(drop=True)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:14.240771Z","iopub.execute_input":"2024-08-05T11:06:14.241426Z","iopub.status.idle":"2024-08-05T11:06:14.257666Z","shell.execute_reply.started":"2024-08-05T11:06:14.241392Z","shell.execute_reply":"2024-08-05T11:06:14.256806Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  Zentropa has much in common with The Third Man...      1\n1  Zentropa is the most original movie I've seen ...      1\n2  Lars Von Trier is never backward in trying out...      1\n3  *Contains spoilers due to me having to describ...      1\n4  That was the first thing that sprang to mind a...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Zentropa has much in common with The Third Man...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Zentropa is the most original movie I've seen ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lars Von Trier is never backward in trying out...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>*Contains spoilers due to me having to describ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>That was the first thing that sprang to mind a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"policy_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\ninit_model = AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\")\n\ninit_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:15.284774Z","iopub.execute_input":"2024-08-05T11:06:15.285134Z","iopub.status.idle":"2024-08-05T11:06:16.814963Z","shell.execute_reply.started":"2024-08-05T11:06:15.285106Z","shell.execute_reply":"2024-08-05T11:06:16.814002Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"reward_model_path = '/kaggle/input/reward2-0/distilbert-imdb/'\n\nreward_tokenizer = AutoTokenizer.from_pretrained(reward_model_path, local_files_only = True)\nreward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path, local_files_only = True)\n\n#reward_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n#reward_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\") \n\nreward_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:16.816888Z","iopub.execute_input":"2024-08-05T11:06:16.817488Z","iopub.status.idle":"2024-08-05T11:06:17.014838Z","shell.execute_reply.started":"2024-08-05T11:06:16.817454Z","shell.execute_reply":"2024-08-05T11:06:17.013937Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# leave first n and tokenize text and create a dataloader\ntokenized = policy_tokenizer(train_df['text'].tolist())\ntokenized = [x[:config['train_first_n']] for x in tokenized['input_ids']]\ntokenized = torch.IntTensor(tokenized)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:17.060682Z","iopub.execute_input":"2024-08-05T11:06:17.060981Z","iopub.status.idle":"2024-08-05T11:06:22.399349Z","shell.execute_reply.started":"2024-08-05T11:06:17.060955Z","shell.execute_reply":"2024-08-05T11:06:22.398583Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized.shape, tokenized[:3]","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.401144Z","iopub.execute_input":"2024-08-05T11:06:22.401444Z","iopub.status.idle":"2024-08-05T11:06:22.408516Z","shell.execute_reply.started":"2024-08-05T11:06:22.401414Z","shell.execute_reply":"2024-08-05T11:06:22.407595Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(torch.Size([12500, 10]),\n tensor([[   57,   298,  1773,    64,   468,   881,   287,  2219,   351,   383],\n         [   57,   298,  1773,    64,   318,   262,   749,  2656,  3807,   314],\n         [   43,   945, 26985,   309,  5277,   318,  1239, 19528,   287,  2111]],\n        dtype=torch.int32))"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"torch.no_grad()\ndef reward_fn_sentiment_imdb(gen_sample, direction = \"pos\"):\n\n    tokens = reward_tokenizer(gen_sample, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n    logits = reward_model(tokens).logits\n    positive_cls = logits.softmax(dim=-1)[:, 1]\n    return positive_cls.to(device)\n\n\n\n# Some samples taken from the IMDB dataset used to finetune this model\nclasses, samples = map(list, zip(*[\n    (\"pos\", \"Just finished watching this movie for maybe the 7th or 8th time, picked it up one night previously viewed at Blockbuster and absolutely loved it, I've shown it to 4 people so far and they have enjoyed it as well.\"),\n    (\"pos\", \"This was the most original movie I've seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days.\"),\n    (\"neg\", \"I can't believe that those praising this movie herein aren't thinking of some other film.\"),\n    (\"neg\", \"This film seemed way too long even at only 75 minutes.\"),\n    (\"neg\", \"Really, I can't believe that I spent $5 on this movie. I am a huge zombie fanatic and thought the movie might be really good. It had zombies in it right? Was I wrong!\"),\n]))\nsentiment = reward_fn_sentiment_imdb(samples).tolist()\nsentiment\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.409798Z","iopub.execute_input":"2024-08-05T11:06:22.410158Z","iopub.status.idle":"2024-08-05T11:06:22.418329Z","shell.execute_reply.started":"2024-08-05T11:06:22.410135Z","shell.execute_reply":"2024-08-05T11:06:22.417502Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'torch.no_grad()\\ndef reward_fn_sentiment_imdb(gen_sample, direction = \"pos\"):\\n\\n    tokens = reward_tokenizer(gen_sample, return_tensors=\\'pt\\', padding=True, truncation=True)[\\'input_ids\\'].to(device)\\n    logits = reward_model(tokens).logits\\n    positive_cls = logits.softmax(dim=-1)[:, 1]\\n    return positive_cls.to(device)\\n\\n\\n\\n# Some samples taken from the IMDB dataset used to finetune this model\\nclasses, samples = map(list, zip(*[\\n    (\"pos\", \"Just finished watching this movie for maybe the 7th or 8th time, picked it up one night previously viewed at Blockbuster and absolutely loved it, I\\'ve shown it to 4 people so far and they have enjoyed it as well.\"),\\n    (\"pos\", \"This was the most original movie I\\'ve seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days.\"),\\n    (\"neg\", \"I can\\'t believe that those praising this movie herein aren\\'t thinking of some other film.\"),\\n    (\"neg\", \"This film seemed way too long even at only 75 minutes.\"),\\n    (\"neg\", \"Really, I can\\'t believe that I spent $5 on this movie. I am a huge zombie fanatic and thought the movie might be really good. It had zombies in it right? Was I wrong!\"),\\n]))\\nsentiment = reward_fn_sentiment_imdb(samples).tolist()\\nsentiment'"},"metadata":{}}]},{"cell_type":"code","source":"train_dataloader = DataLoader(tokenized, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.419468Z","iopub.execute_input":"2024-08-05T11:06:22.419829Z","iopub.status.idle":"2024-08-05T11:06:22.429961Z","shell.execute_reply.started":"2024-08-05T11:06:22.419795Z","shell.execute_reply":"2024-08-05T11:06:22.429079Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def generate(model, idx):\n    idx = idx.to(device)\n    \n    # TODO check each param \n    output = model.generate(idx, max_length = 50, pad_token_id=50256, num_return_sequences = 1, return_dict_in_generate=True, output_scores=True, temperature = temperature)\n    output_ids = output['sequences']\n    generation = policy_tokenizer.batch_decode(output_ids)\n    \n    return output_ids.clone(), generation","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.431745Z","iopub.execute_input":"2024-08-05T11:06:22.432017Z","iopub.status.idle":"2024-08-05T11:06:22.440451Z","shell.execute_reply.started":"2024-08-05T11:06:22.431994Z","shell.execute_reply":"2024-08-05T11:06:22.439641Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def reward_fn_sentiment_imdb(gen_sample):\n    with torch.no_grad():\n        tokens = reward_tokenizer(gen_sample, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n        logits = reward_model(tokens).logits\n        positive_cls = logits.softmax(dim=-1)[:, 1] # TODO CHECK that pos = 1 and neg = 0\n    return positive_cls.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.441557Z","iopub.execute_input":"2024-08-05T11:06:22.441885Z","iopub.status.idle":"2024-08-05T11:06:22.450332Z","shell.execute_reply.started":"2024-08-05T11:06:22.441854Z","shell.execute_reply":"2024-08-05T11:06:22.449388Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_kl(training_logits, ref_logits, beta, first_n):\n    # TODO write log_softmax\n    \n    training_logprobs = training_logits.log_softmax(-1)\n    ref_logprobs = ref_logits.log_softmax(-1)\n\n    probs = training_logprobs.exp()\n    \n    kl = (probs * (training_logprobs - ref_logprobs))[:, first_n:-1].sum(-1)\n    return beta*kl.mean()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.644009Z","iopub.execute_input":"2024-08-05T11:06:22.644357Z","iopub.status.idle":"2024-08-05T11:06:22.650208Z","shell.execute_reply.started":"2024-08-05T11:06:22.644330Z","shell.execute_reply":"2024-08-05T11:06:22.649278Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_generation = \"Before Dogma 95: when Lars used movies as art, not just a story. A beautiful painting about love and death. This is one of my favorite movies of all time. The color... The music... Just perfect.\t\"\n\nrewards = reward_fn_sentiment_imdb(test_generation)\nrewards","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:22.652127Z","iopub.execute_input":"2024-08-05T11:06:22.652485Z","iopub.status.idle":"2024-08-05T11:06:23.324776Z","shell.execute_reply.started":"2024-08-05T11:06:22.652447Z","shell.execute_reply":"2024-08-05T11:06:23.323822Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([0.9939], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"i_iterations = config['I_iterations']\nm_runs = config['M_models']\nt_steps = config['T_steps']\n\nbeta = config['_beta']\nmu = config['mu']\neta = config['eta']\nlmbd = config['lmbd']\n\nfirst_n = config['train_first_n']\ntemperature = config['temperature']","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:23.326558Z","iopub.execute_input":"2024-08-05T11:06:23.327022Z","iopub.status.idle":"2024-08-05T11:06:23.332874Z","shell.execute_reply.started":"2024-08-05T11:06:23.326995Z","shell.execute_reply":"2024-08-05T11:06:23.331740Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#loss_lst, kl_lst, normalized_rewads_lst = [], [], []\ndef t_steps_run(m_model, ema_model_ref):\n    steps = 0\n\n    for batch in train_dataloader:\n\n        tokens, generation = generate(m_model, batch)\n\n        m_model.train()\n        logits = m_model(tokens).logits\n        with torch.no_grad():\n            ref_logits = ema_model_ref(tokens).logits.detach()\n\n        rewards = reward_fn_sentiment_imdb(generation).view(-1, 1, 1) # TODO OR SEPARATE VIEW\n        mean_before_norm = rewards.mean()\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8) # TODO normalize rewards, without this it is instable\n        kl = get_kl(logits, ref_logits, beta, 10)\n        rewards = rewards-kl\n\n\n        log_probs = F.log_softmax(logits[:, config['train_first_n']:, :], dim=-1)\n\n        tokens_generated = tokens[:, config['train_first_n']:]\n\n        selected_log_probs = log_probs.gather(2, tokens_generated.unsqueeze(-1)).squeeze(-1)\n        # handle None\n        selected_log_probs = torch.where(torch.isfinite(selected_log_probs), selected_log_probs, torch.zeros_like(selected_log_probs))\n\n        policy_gradient_loss = (selected_log_probs * rewards).sum(dim=1).mean()\n\n        loss = policy_gradient_loss - kl\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        steps += 1\n        print(f\"Step {steps}, Loss: {loss.item():.4f}, KL: {kl.item():.4f}, Reward: {rewards.mean().item():.4f}\")\n        #loss_lst.append(loss.item())\n        #normalized_rewads_lst.append(rewards.item())\n        #kl_lst.append(kl.item())\n        \n        with torch.no_grad():\n            for m_param, ref_param in zip(m_model.parameters(), ema_model_ref.parameters()):\n                inv_mu = 1-mu\n                ref_param.mul_(inv_mu)\n                b = mu*m_param\n                ref_param.add_(b)\n        if steps >= config['T_steps']:\n            break","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:23.334277Z","iopub.execute_input":"2024-08-05T11:06:23.334655Z","iopub.status.idle":"2024-08-05T11:06:23.349136Z","shell.execute_reply.started":"2024-08-05T11:06:23.334624Z","shell.execute_reply":"2024-08-05T11:06:23.348217Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def liti_init_update(slerp_model, eta):\n    # actual init model is updated TODO CHANGE because we want to go towards SFT on 2nd iteration\n    # or is it? I am not sure\n    with torch.no_grad():\n        for init_param, slerp_param in zip(init_model.parameters(), slerp_model.parameters()):\n            inv_eta = 1-eta\n            init_param.mul_(inv_eta)\n            b = eta * slerp_param\n            init_param.add_(b)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:23.351701Z","iopub.execute_input":"2024-08-05T11:06:23.352044Z","iopub.status.idle":"2024-08-05T11:06:23.364291Z","shell.execute_reply.started":"2024-08-05T11:06:23.352012Z","shell.execute_reply":"2024-08-05T11:06:23.363263Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def slerp(theta_init, theta1, theta2, lmbd):\n    \"\"\"\n    \"\"\"\n    delta1 = theta1 - theta_init\n    delta2 = theta2 - theta_init\n    \n    # Normalize by assumption\n    delta1_norm = torch.norm(delta1)\n    delta2_norm = torch.norm(delta2)\n    delta1_normalized = delta1 / delta1_norm\n    delta2_normalized = delta2 / delta2_norm\n    \n    cos_omega = torch.sum(delta1_normalized * delta2_normalized)\n    cos_omega = torch.clamp(cos_omega, -1.0, 1.0)  # Ensure it's in the valid range for acos\n    omega = torch.acos(cos_omega)\n    \n    sin_omega = torch.sin(omega)\n    \n    if sin_omega.abs() < 1e-6:\n        #If the angle is very small, fall back to linear interpolation, otherwise it is instable\n        return (1 - lmbd) * delta1 + lmbd * delta2\n    else:   \n        term1 = torch.sin((1 - lmbd) * omega) / sin_omega * delta1\n        term2 = torch.sin(lmbd * omega) / sin_omega * delta2\n\n        return term1 + term2","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:23.365513Z","iopub.execute_input":"2024-08-05T11:06:23.365879Z","iopub.status.idle":"2024-08-05T11:06:23.380931Z","shell.execute_reply.started":"2024-08-05T11:06:23.365848Z","shell.execute_reply":"2024-08-05T11:06:23.379954Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for i in range(i_iterations):\n    models_for_slerp = []\n    for run in range(m_runs):\n        m_model = copy.deepcopy(init_model)\n        ema_model_ref = copy.deepcopy(init_model)\n        \n        optimizer = torch.optim.Adam(m_model.parameters(), lr=1e-4)\n\n        # Policy update\n        t_steps_run(m_model, ema_model_ref)\n        torch.cuda.empty_cache()\n        models_for_slerp.append(m_model)\n        \n    # SLERP\n    with torch.no_grad():\n        slerp_model = copy.deepcopy(init_model)\n        for slerp_param, theta1, theta2 in zip(slerp_model.parameters(), models_for_slerp[0].parameters(), models_for_slerp[0].parameters()):\n            slerp_param.add_(slerp(slerp_param, theta1, theta2, lmbd))\n    # LITI\n    liti_init_update(slerp_model, eta)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:06:23.382123Z","iopub.execute_input":"2024-08-05T11:06:23.382397Z","iopub.status.idle":"2024-08-05T11:15:35.834048Z","shell.execute_reply.started":"2024-08-05T11:06:23.382355Z","shell.execute_reply":"2024-08-05T11:15:35.833213Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","output_type":"stream"},{"name":"stdout","text":"Step 1, Loss: 58.0252, KL: 0.0953, Reward: -0.0953\nStep 2, Loss: 172.0777, KL: 0.2375, Reward: -0.2375\nStep 3, Loss: 82.5152, KL: 0.1386, Reward: -0.1386\nStep 4, Loss: 80.5743, KL: 0.1471, Reward: -0.1471\nStep 5, Loss: 59.6340, KL: 0.1123, Reward: -0.1123\nStep 6, Loss: 59.5389, KL: 0.1128, Reward: -0.1128\nStep 7, Loss: 49.6825, KL: 0.1007, Reward: -0.1007\nStep 8, Loss: 48.5116, KL: 0.0988, Reward: -0.0988\nStep 9, Loss: 47.2554, KL: 0.0958, Reward: -0.0958\nStep 10, Loss: 46.4550, KL: 0.0909, Reward: -0.0909\nStep 11, Loss: 42.7089, KL: 0.0828, Reward: -0.0828\nStep 12, Loss: 42.8194, KL: 0.0816, Reward: -0.0816\nStep 13, Loss: 42.4892, KL: 0.0809, Reward: -0.0809\nStep 14, Loss: 44.3916, KL: 0.0858, Reward: -0.0858\nStep 15, Loss: 39.9866, KL: 0.0778, Reward: -0.0778\nStep 16, Loss: 38.9465, KL: 0.0766, Reward: -0.0766\nStep 17, Loss: 36.4349, KL: 0.0722, Reward: -0.0722\nStep 18, Loss: 39.5508, KL: 0.0799, Reward: -0.0799\nStep 19, Loss: 34.5037, KL: 0.0706, Reward: -0.0706\nStep 20, Loss: 34.9179, KL: 0.0706, Reward: -0.0706\nStep 21, Loss: 32.7048, KL: 0.0668, Reward: -0.0668\nStep 22, Loss: 34.4030, KL: 0.0710, Reward: -0.0710\nStep 23, Loss: 35.2145, KL: 0.0714, Reward: -0.0714\nStep 24, Loss: 33.0090, KL: 0.0676, Reward: -0.0676\nStep 25, Loss: 30.5449, KL: 0.0640, Reward: -0.0640\nStep 26, Loss: 32.9886, KL: 0.0686, Reward: -0.0686\nStep 27, Loss: 28.0385, KL: 0.0587, Reward: -0.0587\nStep 28, Loss: 32.2249, KL: 0.0650, Reward: -0.0650\nStep 29, Loss: 28.5655, KL: 0.0591, Reward: -0.0591\nStep 30, Loss: 32.6422, KL: 0.0663, Reward: -0.0663\nStep 31, Loss: 28.1992, KL: 0.0586, Reward: -0.0586\nStep 32, Loss: 28.1755, KL: 0.0594, Reward: -0.0594\nStep 33, Loss: 29.0025, KL: 0.0605, Reward: -0.0605\nStep 34, Loss: 29.8009, KL: 0.0632, Reward: -0.0632\nStep 35, Loss: 27.3103, KL: 0.0575, Reward: -0.0575\nStep 36, Loss: 27.2476, KL: 0.0580, Reward: -0.0580\nStep 37, Loss: 28.0916, KL: 0.0594, Reward: -0.0594\nStep 38, Loss: 28.5137, KL: 0.0603, Reward: -0.0603\nStep 39, Loss: 26.5871, KL: 0.0566, Reward: -0.0566\nStep 40, Loss: 24.8118, KL: 0.0526, Reward: -0.0526\nStep 41, Loss: 26.2619, KL: 0.0548, Reward: -0.0548\nStep 42, Loss: 23.8870, KL: 0.0519, Reward: -0.0519\nStep 43, Loss: 26.8926, KL: 0.0570, Reward: -0.0570\nStep 44, Loss: 25.9523, KL: 0.0546, Reward: -0.0546\nStep 45, Loss: 24.3639, KL: 0.0527, Reward: -0.0527\nStep 46, Loss: 21.0150, KL: 0.0463, Reward: -0.0463\nStep 47, Loss: 27.2200, KL: 0.0589, Reward: -0.0589\nStep 48, Loss: 21.5234, KL: 0.0475, Reward: -0.0475\nStep 49, Loss: 30.2684, KL: 0.0636, Reward: -0.0636\nStep 50, Loss: 22.7988, KL: 0.0495, Reward: -0.0495\nStep 51, Loss: 25.6605, KL: 0.0550, Reward: -0.0550\nStep 52, Loss: 25.2175, KL: 0.0537, Reward: -0.0537\nStep 53, Loss: 21.5287, KL: 0.0471, Reward: -0.0471\nStep 54, Loss: 24.6610, KL: 0.0530, Reward: -0.0530\nStep 55, Loss: 24.0509, KL: 0.0530, Reward: -0.0530\nStep 56, Loss: 23.9404, KL: 0.0521, Reward: -0.0521\nStep 57, Loss: 24.7368, KL: 0.0530, Reward: -0.0530\nStep 58, Loss: 22.1487, KL: 0.0490, Reward: -0.0490\nStep 59, Loss: 26.5175, KL: 0.0575, Reward: -0.0575\nStep 60, Loss: 22.6286, KL: 0.0490, Reward: -0.0490\nStep 61, Loss: 23.9352, KL: 0.0524, Reward: -0.0524\nStep 62, Loss: 21.5481, KL: 0.0485, Reward: -0.0485\nStep 63, Loss: 21.1235, KL: 0.0472, Reward: -0.0472\nStep 64, Loss: 23.0648, KL: 0.0505, Reward: -0.0505\nStep 65, Loss: 21.2328, KL: 0.0472, Reward: -0.0472\nStep 66, Loss: 22.3704, KL: 0.0488, Reward: -0.0488\nStep 67, Loss: 22.7453, KL: 0.0491, Reward: -0.0491\nStep 68, Loss: 24.6300, KL: 0.0536, Reward: -0.0536\nStep 69, Loss: 21.4336, KL: 0.0480, Reward: -0.0480\nStep 70, Loss: 19.9842, KL: 0.0448, Reward: -0.0448\nStep 71, Loss: 20.7382, KL: 0.0460, Reward: -0.0460\nStep 72, Loss: 21.6216, KL: 0.0476, Reward: -0.0476\nStep 73, Loss: 21.4116, KL: 0.0477, Reward: -0.0477\nStep 74, Loss: 22.0827, KL: 0.0496, Reward: -0.0496\nStep 75, Loss: 19.9076, KL: 0.0448, Reward: -0.0448\nStep 76, Loss: 22.6632, KL: 0.0502, Reward: -0.0502\nStep 77, Loss: 20.4970, KL: 0.0464, Reward: -0.0464\nStep 78, Loss: 23.3628, KL: 0.0521, Reward: -0.0521\nStep 79, Loss: 22.8148, KL: 0.0507, Reward: -0.0507\nStep 80, Loss: 20.5580, KL: 0.0460, Reward: -0.0460\nStep 81, Loss: 20.4850, KL: 0.0465, Reward: -0.0465\nStep 82, Loss: 19.6665, KL: 0.0441, Reward: -0.0441\nStep 83, Loss: 20.2267, KL: 0.0454, Reward: -0.0454\nStep 84, Loss: 20.4629, KL: 0.0469, Reward: -0.0469\nStep 85, Loss: 26.0850, KL: 0.0559, Reward: -0.0559\nStep 86, Loss: 18.9929, KL: 0.0426, Reward: -0.0426\nStep 87, Loss: 21.2662, KL: 0.0482, Reward: -0.0482\nStep 88, Loss: 19.5871, KL: 0.0440, Reward: -0.0440\nStep 89, Loss: 19.8508, KL: 0.0452, Reward: -0.0452\nStep 90, Loss: 19.0081, KL: 0.0445, Reward: -0.0445\nStep 91, Loss: 19.7039, KL: 0.0451, Reward: -0.0451\nStep 92, Loss: 18.9260, KL: 0.0435, Reward: -0.0435\nStep 93, Loss: 19.4328, KL: 0.0441, Reward: -0.0441\nStep 94, Loss: 19.0606, KL: 0.0439, Reward: -0.0439\nStep 95, Loss: 21.3343, KL: 0.0475, Reward: -0.0475\nStep 96, Loss: 19.1704, KL: 0.0439, Reward: -0.0439\nStep 97, Loss: 18.7230, KL: 0.0439, Reward: -0.0439\nStep 98, Loss: 20.3779, KL: 0.0469, Reward: -0.0469\nStep 99, Loss: 20.4385, KL: 0.0463, Reward: -0.0463\nStep 100, Loss: 18.2898, KL: 0.0422, Reward: -0.0422\nStep 1, Loss: 66.8603, KL: 0.1107, Reward: -0.1107\nStep 2, Loss: 165.4383, KL: 0.2339, Reward: -0.2339\nStep 3, Loss: 86.1198, KL: 0.1467, Reward: -0.1467\nStep 4, Loss: 87.0468, KL: 0.1607, Reward: -0.1607\nStep 5, Loss: 76.7299, KL: 0.1430, Reward: -0.1430\nStep 6, Loss: 76.6653, KL: 0.1452, Reward: -0.1452\nStep 7, Loss: 53.5440, KL: 0.1044, Reward: -0.1044\nStep 8, Loss: 46.9390, KL: 0.0925, Reward: -0.0925\nStep 9, Loss: 45.9131, KL: 0.0919, Reward: -0.0919\nStep 10, Loss: 44.3824, KL: 0.0880, Reward: -0.0880\nStep 11, Loss: 51.0417, KL: 0.0952, Reward: -0.0952\nStep 12, Loss: 43.6051, KL: 0.0847, Reward: -0.0847\nStep 13, Loss: 42.7730, KL: 0.0838, Reward: -0.0838\nStep 14, Loss: 43.2163, KL: 0.0846, Reward: -0.0846\nStep 15, Loss: 43.4984, KL: 0.0867, Reward: -0.0867\nStep 16, Loss: 40.9150, KL: 0.0802, Reward: -0.0802\nStep 17, Loss: 41.8443, KL: 0.0849, Reward: -0.0849\nStep 18, Loss: 40.2839, KL: 0.0812, Reward: -0.0812\nStep 19, Loss: 35.2996, KL: 0.0726, Reward: -0.0726\nStep 20, Loss: 37.4881, KL: 0.0758, Reward: -0.0758\nStep 21, Loss: 39.5222, KL: 0.0794, Reward: -0.0794\nStep 22, Loss: 32.9413, KL: 0.0675, Reward: -0.0675\nStep 23, Loss: 37.6431, KL: 0.0767, Reward: -0.0767\nStep 24, Loss: 34.3790, KL: 0.0705, Reward: -0.0705\nStep 25, Loss: 32.3582, KL: 0.0675, Reward: -0.0675\nStep 26, Loss: 30.9979, KL: 0.0644, Reward: -0.0644\nStep 27, Loss: 28.4452, KL: 0.0605, Reward: -0.0605\nStep 28, Loss: 32.4721, KL: 0.0683, Reward: -0.0683\nStep 29, Loss: 32.9440, KL: 0.0699, Reward: -0.0699\nStep 30, Loss: 24.9996, KL: 0.0539, Reward: -0.0539\nStep 31, Loss: 28.3829, KL: 0.0602, Reward: -0.0602\nStep 32, Loss: 30.9948, KL: 0.0641, Reward: -0.0641\nStep 33, Loss: 29.4240, KL: 0.0618, Reward: -0.0618\nStep 34, Loss: 29.9732, KL: 0.0633, Reward: -0.0633\nStep 35, Loss: 27.9899, KL: 0.0585, Reward: -0.0585\nStep 36, Loss: 28.5200, KL: 0.0602, Reward: -0.0602\nStep 37, Loss: 25.4816, KL: 0.0538, Reward: -0.0538\nStep 38, Loss: 23.7825, KL: 0.0517, Reward: -0.0517\nStep 39, Loss: 25.5821, KL: 0.0544, Reward: -0.0544\nStep 40, Loss: 25.4084, KL: 0.0537, Reward: -0.0537\nStep 41, Loss: 26.8034, KL: 0.0568, Reward: -0.0568\nStep 42, Loss: 25.7978, KL: 0.0552, Reward: -0.0552\nStep 43, Loss: 26.4319, KL: 0.0567, Reward: -0.0567\nStep 44, Loss: 23.4221, KL: 0.0499, Reward: -0.0499\nStep 45, Loss: 26.4950, KL: 0.0563, Reward: -0.0563\nStep 46, Loss: 24.9045, KL: 0.0537, Reward: -0.0537\nStep 47, Loss: 25.5189, KL: 0.0550, Reward: -0.0550\nStep 48, Loss: 24.2763, KL: 0.0523, Reward: -0.0523\nStep 49, Loss: 25.2696, KL: 0.0545, Reward: -0.0545\nStep 50, Loss: 25.5729, KL: 0.0550, Reward: -0.0550\nStep 51, Loss: 23.2266, KL: 0.0500, Reward: -0.0500\nStep 52, Loss: 26.3211, KL: 0.0562, Reward: -0.0562\nStep 53, Loss: 23.0594, KL: 0.0499, Reward: -0.0499\nStep 54, Loss: 23.0318, KL: 0.0500, Reward: -0.0500\nStep 55, Loss: 23.2467, KL: 0.0496, Reward: -0.0496\nStep 56, Loss: 25.3854, KL: 0.0533, Reward: -0.0533\nStep 57, Loss: 24.6424, KL: 0.0534, Reward: -0.0534\nStep 58, Loss: 24.5386, KL: 0.0534, Reward: -0.0534\nStep 59, Loss: 20.2626, KL: 0.0444, Reward: -0.0444\nStep 60, Loss: 20.6474, KL: 0.0453, Reward: -0.0453\nStep 61, Loss: 20.8241, KL: 0.0461, Reward: -0.0461\nStep 62, Loss: 24.8035, KL: 0.0524, Reward: -0.0524\nStep 63, Loss: 21.3877, KL: 0.0474, Reward: -0.0474\nStep 64, Loss: 23.1855, KL: 0.0504, Reward: -0.0504\nStep 65, Loss: 24.1259, KL: 0.0522, Reward: -0.0522\nStep 66, Loss: 21.0158, KL: 0.0463, Reward: -0.0463\nStep 67, Loss: 23.0137, KL: 0.0498, Reward: -0.0498\nStep 68, Loss: 22.7607, KL: 0.0496, Reward: -0.0496\nStep 69, Loss: 22.9272, KL: 0.0500, Reward: -0.0500\nStep 70, Loss: 21.3517, KL: 0.0467, Reward: -0.0467\nStep 71, Loss: 19.1207, KL: 0.0428, Reward: -0.0428\nStep 72, Loss: 22.8284, KL: 0.0505, Reward: -0.0505\nStep 73, Loss: 21.0074, KL: 0.0458, Reward: -0.0458\nStep 74, Loss: 21.6591, KL: 0.0485, Reward: -0.0485\nStep 75, Loss: 22.4285, KL: 0.0496, Reward: -0.0496\nStep 76, Loss: 20.6641, KL: 0.0463, Reward: -0.0463\nStep 77, Loss: 21.8285, KL: 0.0488, Reward: -0.0488\nStep 78, Loss: 20.3364, KL: 0.0452, Reward: -0.0452\nStep 79, Loss: 22.8419, KL: 0.0497, Reward: -0.0497\nStep 80, Loss: 20.0196, KL: 0.0446, Reward: -0.0446\nStep 81, Loss: 20.9724, KL: 0.0464, Reward: -0.0464\nStep 82, Loss: 20.9303, KL: 0.0461, Reward: -0.0461\nStep 83, Loss: 18.6410, KL: 0.0425, Reward: -0.0425\nStep 84, Loss: 17.9626, KL: 0.0409, Reward: -0.0409\nStep 85, Loss: 19.8316, KL: 0.0445, Reward: -0.0445\nStep 86, Loss: 20.2145, KL: 0.0457, Reward: -0.0457\nStep 87, Loss: 18.8513, KL: 0.0423, Reward: -0.0423\nStep 88, Loss: 19.3316, KL: 0.0434, Reward: -0.0434\nStep 89, Loss: 21.8010, KL: 0.0479, Reward: -0.0479\nStep 90, Loss: 19.8974, KL: 0.0452, Reward: -0.0452\nStep 91, Loss: 19.9972, KL: 0.0452, Reward: -0.0452\nStep 92, Loss: 20.8705, KL: 0.0459, Reward: -0.0459\nStep 93, Loss: 19.9095, KL: 0.0449, Reward: -0.0449\nStep 94, Loss: 20.6155, KL: 0.0472, Reward: -0.0472\nStep 95, Loss: 20.7863, KL: 0.0467, Reward: -0.0467\nStep 96, Loss: 20.2433, KL: 0.0460, Reward: -0.0460\nStep 97, Loss: 19.7882, KL: 0.0451, Reward: -0.0451\nStep 98, Loss: 21.5849, KL: 0.0492, Reward: -0.0492\nStep 99, Loss: 21.0048, KL: 0.0473, Reward: -0.0473\nStep 100, Loss: 20.3012, KL: 0.0457, Reward: -0.0457\nStep 1, Loss: 26.9409, KL: 0.0521, Reward: -0.0521\nStep 2, Loss: 52.9824, KL: 0.1002, Reward: -0.1002\nStep 3, Loss: 31.7003, KL: 0.0646, Reward: -0.0646\nStep 4, Loss: 37.2757, KL: 0.0805, Reward: -0.0805\nStep 5, Loss: 37.3544, KL: 0.0810, Reward: -0.0810\nStep 6, Loss: 32.5373, KL: 0.0709, Reward: -0.0709\nStep 7, Loss: 29.2126, KL: 0.0632, Reward: -0.0632\nStep 8, Loss: 28.1876, KL: 0.0613, Reward: -0.0613\nStep 9, Loss: 32.7397, KL: 0.0693, Reward: -0.0693\nStep 10, Loss: 31.0020, KL: 0.0675, Reward: -0.0675\nStep 11, Loss: 28.3501, KL: 0.0620, Reward: -0.0620\nStep 12, Loss: 26.1386, KL: 0.0572, Reward: -0.0572\nStep 13, Loss: 26.4897, KL: 0.0579, Reward: -0.0579\nStep 14, Loss: 25.4889, KL: 0.0576, Reward: -0.0576\nStep 15, Loss: 21.9455, KL: 0.0507, Reward: -0.0507\nStep 16, Loss: 23.9476, KL: 0.0554, Reward: -0.0554\nStep 17, Loss: 24.0640, KL: 0.0545, Reward: -0.0545\nStep 18, Loss: 22.9782, KL: 0.0533, Reward: -0.0533\nStep 19, Loss: 23.5441, KL: 0.0537, Reward: -0.0537\nStep 20, Loss: 24.3937, KL: 0.0548, Reward: -0.0548\nStep 21, Loss: 24.5890, KL: 0.0568, Reward: -0.0568\nStep 22, Loss: 24.5892, KL: 0.0559, Reward: -0.0559\nStep 23, Loss: 23.1843, KL: 0.0516, Reward: -0.0516\nStep 24, Loss: 22.7233, KL: 0.0509, Reward: -0.0509\nStep 25, Loss: 23.0526, KL: 0.0528, Reward: -0.0528\nStep 26, Loss: 22.0265, KL: 0.0496, Reward: -0.0496\nStep 27, Loss: 18.8714, KL: 0.0436, Reward: -0.0436\nStep 28, Loss: 20.8690, KL: 0.0477, Reward: -0.0477\nStep 29, Loss: 21.7045, KL: 0.0505, Reward: -0.0505\nStep 30, Loss: 19.5750, KL: 0.0460, Reward: -0.0460\nStep 31, Loss: 21.3959, KL: 0.0495, Reward: -0.0495\nStep 32, Loss: 23.1978, KL: 0.0539, Reward: -0.0539\nStep 33, Loss: 19.6311, KL: 0.0452, Reward: -0.0452\nStep 34, Loss: 20.2967, KL: 0.0469, Reward: -0.0469\nStep 35, Loss: 19.8800, KL: 0.0457, Reward: -0.0457\nStep 36, Loss: 17.6793, KL: 0.0421, Reward: -0.0421\nStep 37, Loss: 20.9148, KL: 0.0479, Reward: -0.0479\nStep 38, Loss: 19.2955, KL: 0.0444, Reward: -0.0444\nStep 39, Loss: 19.4500, KL: 0.0460, Reward: -0.0460\nStep 40, Loss: 17.5503, KL: 0.0411, Reward: -0.0411\nStep 41, Loss: 20.9601, KL: 0.0487, Reward: -0.0487\nStep 42, Loss: 19.3464, KL: 0.0446, Reward: -0.0446\nStep 43, Loss: 18.2793, KL: 0.0423, Reward: -0.0423\nStep 44, Loss: 19.4678, KL: 0.0452, Reward: -0.0452\nStep 45, Loss: 19.5002, KL: 0.0456, Reward: -0.0456\nStep 46, Loss: 19.2128, KL: 0.0456, Reward: -0.0456\nStep 47, Loss: 17.9186, KL: 0.0419, Reward: -0.0419\nStep 48, Loss: 17.8517, KL: 0.0417, Reward: -0.0417\nStep 49, Loss: 20.9130, KL: 0.0496, Reward: -0.0496\nStep 50, Loss: 18.0426, KL: 0.0422, Reward: -0.0422\nStep 51, Loss: 18.1157, KL: 0.0432, Reward: -0.0432\nStep 52, Loss: 19.7807, KL: 0.0470, Reward: -0.0470\nStep 53, Loss: 18.1303, KL: 0.0429, Reward: -0.0429\nStep 54, Loss: 17.3698, KL: 0.0412, Reward: -0.0412\nStep 55, Loss: 18.3624, KL: 0.0423, Reward: -0.0423\nStep 56, Loss: 20.6302, KL: 0.0484, Reward: -0.0484\nStep 57, Loss: 15.9423, KL: 0.0386, Reward: -0.0386\nStep 58, Loss: 18.0646, KL: 0.0433, Reward: -0.0433\nStep 59, Loss: 17.3913, KL: 0.0417, Reward: -0.0417\nStep 60, Loss: 18.4679, KL: 0.0439, Reward: -0.0439\nStep 61, Loss: 17.2340, KL: 0.0416, Reward: -0.0416\nStep 62, Loss: 16.6742, KL: 0.0400, Reward: -0.0400\nStep 63, Loss: 18.1371, KL: 0.0428, Reward: -0.0428\nStep 64, Loss: 14.8080, KL: 0.0361, Reward: -0.0361\nStep 65, Loss: 16.8845, KL: 0.0408, Reward: -0.0408\nStep 66, Loss: 17.5858, KL: 0.0423, Reward: -0.0423\nStep 67, Loss: 16.0871, KL: 0.0390, Reward: -0.0390\nStep 68, Loss: 18.0057, KL: 0.0428, Reward: -0.0428\nStep 69, Loss: 17.7890, KL: 0.0429, Reward: -0.0429\nStep 70, Loss: 15.9850, KL: 0.0387, Reward: -0.0387\nStep 71, Loss: 18.4969, KL: 0.0443, Reward: -0.0443\nStep 72, Loss: 17.9085, KL: 0.0434, Reward: -0.0434\nStep 73, Loss: 17.0924, KL: 0.0420, Reward: -0.0420\nStep 74, Loss: 16.4992, KL: 0.0398, Reward: -0.0398\nStep 75, Loss: 19.0837, KL: 0.0461, Reward: -0.0461\nStep 76, Loss: 17.6218, KL: 0.0434, Reward: -0.0434\nStep 77, Loss: 16.8123, KL: 0.0412, Reward: -0.0412\nStep 78, Loss: 15.8577, KL: 0.0397, Reward: -0.0397\nStep 79, Loss: 18.6100, KL: 0.0460, Reward: -0.0460\nStep 80, Loss: 16.3195, KL: 0.0408, Reward: -0.0408\nStep 81, Loss: 16.9426, KL: 0.0416, Reward: -0.0416\nStep 82, Loss: 16.2245, KL: 0.0409, Reward: -0.0409\nStep 83, Loss: 16.4097, KL: 0.0403, Reward: -0.0403\nStep 84, Loss: 17.0206, KL: 0.0419, Reward: -0.0419\nStep 85, Loss: 16.9023, KL: 0.0416, Reward: -0.0416\nStep 86, Loss: 14.5569, KL: 0.0358, Reward: -0.0358\nStep 87, Loss: 17.0353, KL: 0.0422, Reward: -0.0422\nStep 88, Loss: 17.9250, KL: 0.0429, Reward: -0.0429\nStep 89, Loss: 16.7894, KL: 0.0423, Reward: -0.0423\nStep 90, Loss: 16.0424, KL: 0.0394, Reward: -0.0394\nStep 91, Loss: 15.5795, KL: 0.0380, Reward: -0.0380\nStep 92, Loss: 14.0864, KL: 0.0357, Reward: -0.0357\nStep 93, Loss: 16.4716, KL: 0.0410, Reward: -0.0410\nStep 94, Loss: 15.4955, KL: 0.0386, Reward: -0.0386\nStep 95, Loss: 16.4385, KL: 0.0409, Reward: -0.0409\nStep 96, Loss: 18.0806, KL: 0.0433, Reward: -0.0433\nStep 97, Loss: 15.9979, KL: 0.0398, Reward: -0.0398\nStep 98, Loss: 15.4090, KL: 0.0378, Reward: -0.0378\nStep 99, Loss: 16.3286, KL: 0.0409, Reward: -0.0409\nStep 100, Loss: 17.6483, KL: 0.0441, Reward: -0.0441\nStep 1, Loss: 26.2496, KL: 0.0503, Reward: -0.0503\nStep 2, Loss: 55.4556, KL: 0.1043, Reward: -0.1043\nStep 3, Loss: 26.3284, KL: 0.0564, Reward: -0.0564\nStep 4, Loss: 35.7919, KL: 0.0776, Reward: -0.0776\nStep 5, Loss: 33.4333, KL: 0.0743, Reward: -0.0743\nStep 6, Loss: 34.2270, KL: 0.0756, Reward: -0.0756\nStep 7, Loss: 26.5786, KL: 0.0609, Reward: -0.0609\nStep 8, Loss: 30.3901, KL: 0.0660, Reward: -0.0660\nStep 9, Loss: 28.6669, KL: 0.0611, Reward: -0.0611\nStep 10, Loss: 27.2211, KL: 0.0587, Reward: -0.0587\nStep 11, Loss: 29.4383, KL: 0.0628, Reward: -0.0628\nStep 12, Loss: 25.3898, KL: 0.0552, Reward: -0.0552\nStep 13, Loss: 23.9386, KL: 0.0537, Reward: -0.0537\nStep 14, Loss: 25.9954, KL: 0.0587, Reward: -0.0587\nStep 15, Loss: 25.1291, KL: 0.0575, Reward: -0.0575\nStep 16, Loss: 23.2840, KL: 0.0542, Reward: -0.0542\nStep 17, Loss: 28.6049, KL: 0.0639, Reward: -0.0639\nStep 18, Loss: 23.6088, KL: 0.0548, Reward: -0.0548\nStep 19, Loss: 25.9713, KL: 0.0593, Reward: -0.0593\nStep 20, Loss: 26.9795, KL: 0.0614, Reward: -0.0614\nStep 21, Loss: 24.7085, KL: 0.0563, Reward: -0.0563\nStep 22, Loss: 23.1146, KL: 0.0517, Reward: -0.0517\nStep 23, Loss: 23.1420, KL: 0.0524, Reward: -0.0524\nStep 24, Loss: 23.1101, KL: 0.0521, Reward: -0.0521\nStep 25, Loss: 22.3008, KL: 0.0507, Reward: -0.0507\nStep 26, Loss: 23.1104, KL: 0.0521, Reward: -0.0521\nStep 27, Loss: 19.9018, KL: 0.0462, Reward: -0.0462\nStep 28, Loss: 21.1161, KL: 0.0489, Reward: -0.0489\nStep 29, Loss: 22.3821, KL: 0.0521, Reward: -0.0521\nStep 30, Loss: 22.3212, KL: 0.0523, Reward: -0.0523\nStep 31, Loss: 20.4519, KL: 0.0492, Reward: -0.0492\nStep 32, Loss: 21.8710, KL: 0.0521, Reward: -0.0521\nStep 33, Loss: 19.5668, KL: 0.0454, Reward: -0.0454\nStep 34, Loss: 17.6910, KL: 0.0415, Reward: -0.0415\nStep 35, Loss: 18.9901, KL: 0.0449, Reward: -0.0449\nStep 36, Loss: 19.9233, KL: 0.0462, Reward: -0.0462\nStep 37, Loss: 21.9312, KL: 0.0502, Reward: -0.0502\nStep 38, Loss: 18.9532, KL: 0.0444, Reward: -0.0444\nStep 39, Loss: 20.8224, KL: 0.0478, Reward: -0.0478\nStep 40, Loss: 21.8166, KL: 0.0501, Reward: -0.0501\nStep 41, Loss: 17.8192, KL: 0.0421, Reward: -0.0421\nStep 42, Loss: 20.1203, KL: 0.0478, Reward: -0.0478\nStep 43, Loss: 19.0844, KL: 0.0451, Reward: -0.0451\nStep 44, Loss: 19.0397, KL: 0.0450, Reward: -0.0450\nStep 45, Loss: 17.2980, KL: 0.0419, Reward: -0.0419\nStep 46, Loss: 21.6366, KL: 0.0498, Reward: -0.0498\nStep 47, Loss: 21.3924, KL: 0.0496, Reward: -0.0496\nStep 48, Loss: 18.8070, KL: 0.0444, Reward: -0.0444\nStep 49, Loss: 18.6638, KL: 0.0435, Reward: -0.0435\nStep 50, Loss: 18.5009, KL: 0.0432, Reward: -0.0432\nStep 51, Loss: 17.5422, KL: 0.0420, Reward: -0.0420\nStep 52, Loss: 17.9139, KL: 0.0421, Reward: -0.0421\nStep 53, Loss: 18.8925, KL: 0.0448, Reward: -0.0448\nStep 54, Loss: 18.7692, KL: 0.0446, Reward: -0.0446\nStep 55, Loss: 16.0613, KL: 0.0392, Reward: -0.0392\nStep 56, Loss: 18.1268, KL: 0.0438, Reward: -0.0438\nStep 57, Loss: 19.4231, KL: 0.0463, Reward: -0.0463\nStep 58, Loss: 16.1855, KL: 0.0396, Reward: -0.0396\nStep 59, Loss: 18.3054, KL: 0.0431, Reward: -0.0431\nStep 60, Loss: 21.2506, KL: 0.0490, Reward: -0.0490\nStep 61, Loss: 20.2407, KL: 0.0474, Reward: -0.0474\nStep 62, Loss: 20.1204, KL: 0.0470, Reward: -0.0470\nStep 63, Loss: 20.2589, KL: 0.0475, Reward: -0.0475\nStep 64, Loss: 17.5919, KL: 0.0429, Reward: -0.0429\nStep 65, Loss: 17.5572, KL: 0.0423, Reward: -0.0423\nStep 66, Loss: 16.5232, KL: 0.0405, Reward: -0.0405\nStep 67, Loss: 16.8994, KL: 0.0405, Reward: -0.0405\nStep 68, Loss: 17.6370, KL: 0.0431, Reward: -0.0431\nStep 69, Loss: 21.2736, KL: 0.0495, Reward: -0.0495\nStep 70, Loss: 18.5132, KL: 0.0447, Reward: -0.0447\nStep 71, Loss: 17.5927, KL: 0.0425, Reward: -0.0425\nStep 72, Loss: 18.1715, KL: 0.0434, Reward: -0.0434\nStep 73, Loss: 17.3452, KL: 0.0418, Reward: -0.0418\nStep 74, Loss: 19.7171, KL: 0.0472, Reward: -0.0472\nStep 75, Loss: 15.8101, KL: 0.0395, Reward: -0.0395\nStep 76, Loss: 17.4075, KL: 0.0426, Reward: -0.0426\nStep 77, Loss: 16.1072, KL: 0.0394, Reward: -0.0394\nStep 78, Loss: 17.2152, KL: 0.0417, Reward: -0.0417\nStep 79, Loss: 18.3922, KL: 0.0449, Reward: -0.0449\nStep 80, Loss: 18.9652, KL: 0.0460, Reward: -0.0460\nStep 81, Loss: 17.4001, KL: 0.0419, Reward: -0.0419\nStep 82, Loss: 15.8038, KL: 0.0397, Reward: -0.0397\nStep 83, Loss: 17.8435, KL: 0.0437, Reward: -0.0437\nStep 84, Loss: 17.2053, KL: 0.0427, Reward: -0.0427\nStep 85, Loss: 17.1974, KL: 0.0418, Reward: -0.0418\nStep 86, Loss: 17.3770, KL: 0.0428, Reward: -0.0428\nStep 87, Loss: 16.6225, KL: 0.0413, Reward: -0.0413\nStep 88, Loss: 16.6760, KL: 0.0420, Reward: -0.0420\nStep 89, Loss: 20.1878, KL: 0.0500, Reward: -0.0500\nStep 90, Loss: 15.5685, KL: 0.0391, Reward: -0.0391\nStep 91, Loss: 15.7171, KL: 0.0395, Reward: -0.0395\nStep 92, Loss: 15.9002, KL: 0.0397, Reward: -0.0397\nStep 93, Loss: 15.0926, KL: 0.0377, Reward: -0.0377\nStep 94, Loss: 15.1309, KL: 0.0373, Reward: -0.0373\nStep 95, Loss: 14.9004, KL: 0.0367, Reward: -0.0367\nStep 96, Loss: 16.0144, KL: 0.0394, Reward: -0.0394\nStep 97, Loss: 15.8235, KL: 0.0398, Reward: -0.0398\nStep 98, Loss: 16.6771, KL: 0.0408, Reward: -0.0408\nStep 99, Loss: 16.5163, KL: 0.0407, Reward: -0.0407\nStep 100, Loss: 14.9226, KL: 0.0385, Reward: -0.0385\n","output_type":"stream"}]},{"cell_type":"code","source":"init_model.save_pretrained('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:16:54.554907Z","iopub.execute_input":"2024-08-05T11:16:54.555482Z","iopub.status.idle":"2024-08-05T11:16:55.600117Z","shell.execute_reply.started":"2024-08-05T11:16:54.555445Z","shell.execute_reply":"2024-08-05T11:16:55.599045Z"},"trusted":true},"execution_count":22,"outputs":[]}]}